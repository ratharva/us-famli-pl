{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/raid/home/ayrisbud/anaconda3/envs/torch_us/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/raid/home/ayrisbud/anaconda3/envs/torch_us/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/mnt/raid/home/ayrisbud/anaconda3/envs/torch_us/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/mnt/raid/home/ayrisbud/anaconda3/envs/torch_us/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/mnt/raid/home/ayrisbud/anaconda3/envs/torch_us/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/mnt/raid/home/ayrisbud/anaconda3/envs/torch_us/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from torch.autograd import Function\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "\n",
    "# from loaders.ultrasound_dataset import USDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from loaders.ultrasound_dataset_classification import USDataset\n",
    "from transforms.ultrasound_transforms import USClassEvalTransforms\n",
    "from transferModel import EfficientNetTransfer\n",
    "# from transforms.ultrasound_transforms import USEvalTransforms\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "from captum.attr import GuidedGradCam, GuidedBackprop\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import visualization as viz\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from torchvision import transforms\n",
    "from monai.transforms import ScaleIntensityRange\n",
    "\n",
    "from pl_bolts.transforms.dataset_normalizations import (\n",
    "    imagenet_normalization\n",
    ")\n",
    "import nrrd\n",
    "from PIL import Image\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import cv2\n",
    "# from matplotlib.backends.backend_qt4agg import FigureCanvasQTAgg as FigureCanvas\n",
    "from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\n",
    "\n",
    "import pydicom\n",
    "from sklearn.model_selection import train_test_split\n",
    "import uuid\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figArr(fig, draw=True):\n",
    "    fig.set_facecolor(\"black\")\n",
    "    myCanvas = fig.canvas#FigureCanvas(fig)\n",
    "    myCanvas.draw()\n",
    "    w, h = myCanvas.get_width_height()#fig.get_size_inches() * fig.get_dpi()\n",
    "    myArr = np.frombuffer(myCanvas.tostring_rgb(), dtype=np.uint8).reshape(h, w, 3)\n",
    "    myImg = cv2.cvtColor(myArr, cv2.COLOR_RGB2BGR)\n",
    "    # plt_fig.grid()\n",
    "    return myImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GuidedBackpropReLU(Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, input):\n",
    "#         positive_mask = (input > 0).type_as(input)\n",
    "#         ctx.save_for_backward(input, positive_mask)\n",
    "#         return input * positive_mask\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         input, positive_mask = ctx.saved_tensors\n",
    "#         grad_input = grad_output * positive_mask\n",
    "#         return grad_input\n",
    "\n",
    "# class GuidedGradCAMMan:\n",
    "#     def __init__(self, model, target_layer):\n",
    "#         self.model = model\n",
    "#         self.target_layer = target_layer\n",
    "#         self.gradients = None\n",
    "#         self.target_layer_output = None\n",
    "\n",
    "#         self.model.eval()\n",
    "#         self.register_hooks()\n",
    "\n",
    "#     def register_hooks(self):\n",
    "#         def hook_fn(module, input, output):\n",
    "#             self.target_layer_output = output\n",
    "\n",
    "#         target_layer = self.target_layer\n",
    "#         for _, module in self.model.named_modules():\n",
    "#             if isinstance(module, nn.ReLU):\n",
    "#                 module.register_backward_hook(GuidedBackpropReLU.backward)\n",
    "#             if str(module) == target_layer:\n",
    "#                 module.register_forward_hook(hook_fn)\n",
    "\n",
    "#     def generate(self, model_output, target_class):\n",
    "#         # input_tensor.requires_grad = True\n",
    "#         # model_output = self.model(input_tensor)\n",
    "#         print(\"Generate1\")\n",
    "#         self.model.zero_grad()\n",
    "#         print(\"Generate1.5\")\n",
    "#         one_hot_output = torch.zeros_like(model_output)\n",
    "#         print(\"Generate1.6\")\n",
    "#         one_hot_output[0][target_class] = 1\n",
    "#         print(\"Generate1.7\")\n",
    "#         print(\"OHE: \", one_hot_output)\n",
    "#         model_output.backward(gradient=one_hot_output)\n",
    "#         print(\"Generate2\")\n",
    "#         print(\"Gradients: \", self.gradients)\n",
    "#         gradients = self.gradients\n",
    "#         guided_gradients = gradients.cpu().numpy()[0]\n",
    "\n",
    "#         print(\"Generate3\")\n",
    "\n",
    "#         target_layer_output = self.target_layer_output.cpu().detach().numpy()[0]\n",
    "#         guided_gradcam = guided_gradients * target_layer_output\n",
    "#         guided_gradcam = np.maximum(guided_gradcam, 0)\n",
    "#         guided_gradcam = guided_gradcam / guided_gradcam.max()\n",
    "#         print(\"Generate4\")\n",
    "\n",
    "#         return guided_gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def runGGC(myMountPoint, myCSV, myImgCol, myBatchSize, myNumWorkers, myNn, myModel):\n",
    "#     if(os.path.splitext(myCSV)[1] == \".csv\"):        \n",
    "#             df_test = pd.read_csv(os.path.join(myMountPoint, myCSV))\n",
    "#     else:        \n",
    "#         df_test = pd.read_parquet(os.path.join(myMountPoint, myCSV))\n",
    "            \n",
    "#     test_ds = USDataset(df_test, label_column = None, img_column=myImgCol, transform=USClassEvalTransforms(), mount_point=myMountPoint)\n",
    "#     test_loader = DataLoader(test_ds, batch_size=myBatchSize, shuffle=False, num_workers=myNumWorkers, pin_memory=True, prefetch_factor=4)\n",
    "#     model = EfficientNetTransfer(base_encoder=myNn, ckpt_path=\"/mnt/raid/C1_ML_Analysis/train_output/classification/extract_frames_blind_sweeps_c1_30082022_wscores_train_train_sample_clean_feat/epoch=9-val_loss=0.27.ckpt\").load_from_checkpoint(myModel)\n",
    "    \n",
    "#     model.eval()\n",
    "#     model.cuda()\n",
    "#     target_layer_name = model.efficientnet.convnet.features[8][0]\n",
    "#     guidedGradcam = GuidedGradCAMMan(model, target_layer_name)\n",
    "#     # with torch.no_grad():\n",
    "#     predictions = []\n",
    "#     probs = []\n",
    "#     features = []\n",
    "#     pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "#     # for idx, X in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "#     for idx, X in pbar:\n",
    "#         X = X.cuda().contiguous()\n",
    "#         myPred = model(X)\n",
    "#         myPredSigmoid = nn.Softmax(dim=1)(myPred)\n",
    "#         converted_tensor = torch.where(myPredSigmoid >= 0.08, torch.tensor(1), torch.tensor(0))\n",
    "#         converted_tensor = converted_tensor.cpu().numpy()\n",
    "#         print(\"ConvertedTensor: \", converted_tensor)\n",
    "#         print(\"ConvertedTensorType: \", type(converted_tensor))\n",
    "#         toRunIndices = [index for index, value in enumerate(converted_tensor[0]) if value == 1]\n",
    "#         for i in range(len(toRunIndices)):\n",
    "#             print(\"EnteredFor: \", str(i))\n",
    "#             heatmap = guidedGradcam.generate(myPred, toRunIndices[i])\n",
    "#             print(\"CompletedHeatmapGeneration: \", str(i))\n",
    "#             plt.imshow(heatmap, cmap='viridis')\n",
    "#             plt.axis('off')\n",
    "#             plt.title('Guided Grad-CAM Heatmap')\n",
    "#             saveFigPath = \"/mnt/raid/home/ayrisbud/us-famli-pl/src/gradcamOutput/tempFig\" + str(i) + \".png\"\n",
    "#             plt.savefig(saveFigPath)\n",
    "                        \n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myCSV = \"/mnt/raid/home/ayrisbud/us-famli-pl/src/annotatedTrainConcise.csv\"\n",
    "# myModel = \"/mnt/raid/home/ayrisbud/train_output/classification/epoch=21-val_loss=1.06.ckpt\"\n",
    "# myImgCol = \"img_path\"\n",
    "# myClassCol = \"pred_cluster\"\n",
    "# myNn = \"efficientnet_b0\"    \n",
    "# myOutFile = \"./myOutput\"\n",
    "# myBatchSize = 1\n",
    "# myNumWorkers = 16\n",
    "# myMountPoint = \"/mnt/raid/C1_ML_Analysis/\"\n",
    "# myExtractFeatures = False\n",
    "# myLabels = True\n",
    "# runGGC(myMountPoint, myCSV, myImgCol, myBatchSize, myNumWorkers, myNn, myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3465051350.py, line 127)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [8], line 127\u001b[0;36m\u001b[0m\n\u001b[0;31m    sign=\"all\"\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "def useGradcam(myCSV, myModel, myImgCol, myLabels, myNn, myOutFile, myBatchSize, myNumWorkers, myMountPoint, myExtractFeatures):\n",
    "    model = EfficientNetTransfer(base_encoder=myNn, ckpt_path=\"/mnt/raid/C1_ML_Analysis/train_output/classification/extract_frames_blind_sweeps_c1_30082022_wscores_train_train_sample_clean_feat/epoch=9-val_loss=0.27.ckpt\").load_from_checkpoint(myModel)\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    # print(model)\n",
    "    myGuidedGradCam = GuidedGradCam(model, model.efficientnet.convnet.features[8][0])\n",
    "\n",
    "    myLabelList = ['No structures visible', 'Head Visible',\n",
    "                'Abdomen Visible', 'Amniotic fluid visible', \n",
    "                'Placenta visible', 'Fetus or CRL visible']\n",
    "    if myExtractFeatures:\n",
    "            model.extract_features = True\n",
    "\n",
    "    if(os.path.splitext(myCSV)[1] == \".csv\"):        \n",
    "        df_test = pd.read_csv(os.path.join(myMountPoint, myCSV))\n",
    "    else:        \n",
    "        df_test = pd.read_parquet(os.path.join(myMountPoint, myCSV))\n",
    "        \n",
    "    test_ds = USDataset(df_test, label_column = None, img_column=myImgCol, transform=USClassEvalTransforms(), mount_point=myMountPoint)\n",
    "    test_loader = DataLoader(test_ds, batch_size=myBatchSize, shuffle=False, num_workers=myNumWorkers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "\n",
    "        # testImg = USDataset.__getitem__(idx=1)\n",
    "        # print(testImg)\n",
    "        # test_loader = DataLoader(test_ds, batch_size=myBatchSize, shuffle=False, num_workers=myNumWorkers, pin_memory=True, prefetch_factor=4)\n",
    "        # transform_test_loader = DataLoader(test_ds, batch_size=myBatchSize, shuffle=False, num_workers=myNumWorkers, pin_memory=True, prefetch_factor=4)\n",
    "        # print(test_loader.__getitem__(1))\n",
    "        # print(len(test_loader))\n",
    "        \n",
    "\n",
    "    # with torch.no_grad():\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "    # transforms.Resize(256),\n",
    "    # transforms.CenterCrop(256),\n",
    "    # transforms.ToTensor()\n",
    "    transforms.CenterCrop(256),\n",
    "    ScaleIntensityRange(a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0),\n",
    "    # imagenet_normalization(),\n",
    "    ])\n",
    "    predictions = []\n",
    "    probs = []\n",
    "    features = []\n",
    "    pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    # for idx, X in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "    for idx, X in pbar:\n",
    "        # if use_class_column:\n",
    "        #     # print(\"Does it enter this if?\")\n",
    "        #     # X, Y = X\n",
    "        #     # print(type(Y))\n",
    "        #     imgTransform = transform(X)\n",
    "        # print((X))\n",
    "        # print(X.shape)\n",
    "        if myLabels:\n",
    "            imgTransform = transform(X)\n",
    "        # print(X)\n",
    "        X = X.cuda().contiguous()   \n",
    "        if myExtractFeatures:\n",
    "            # print(\"Does it enter this if2?\")        \n",
    "            pred, x_f = model(X)    \n",
    "            features.append(x_f.cpu().numpy())\n",
    "        else:\n",
    "            # print(\"Does it enter this else?\")\n",
    "            pred = model(X)\n",
    "        # print(pred)\n",
    "        # mySoftmax = softmax(pred.cpu().numpy())\n",
    "        # sortedArray = np.sort(mySoftmax)[0][::-1]\n",
    "\n",
    "        # sortedIndices = np.argsort(mySoftmax)[0][::-1]\n",
    "\n",
    "        # pred = torch.argmax(pred).cpu().numpy()\n",
    "        myPredSigmoid = nn.Softmax(dim=1)(pred)\n",
    "        converted_tensor = torch.where(myPredSigmoid >= 0.08, torch.tensor(1), torch.tensor(0))\n",
    "        converted_tensor = converted_tensor.cuda()\n",
    "        converted_tensor = np.array(converted_tensor.cpu())\n",
    "        converted_tensor = converted_tensor[0][1:]\n",
    "        print(\"Converted Tensor\", converted_tensor)\n",
    "        # print(pred)\n",
    "        # for preds in converted_tensor[0]:\n",
    "        isInArray = np.any(converted_tensor == 1)\n",
    "        print(isInArray)\n",
    "        if isInArray:\n",
    "            oriImag = np.transpose(X.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "            print(\"OriImageShape: \", oriImag.shape)\n",
    "\n",
    "            fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "            # Add a subplot to the figure\n",
    "            ax = fig.add_subplot(111)\n",
    "\n",
    "            # Display the oriImag using imshow on the subplot\n",
    "            img_arr = ax.imshow(oriImag)\n",
    "            # print(img_arr)\n",
    "\n",
    "            # plt.imshow(oriImag)\n",
    "            # plt.show()\n",
    "            # break\n",
    "            originalImage = figArr(fig)\n",
    "            tempImage = None\n",
    "            myDict = {}\n",
    "        print(\"Length of convertedArray: \", len(converted_tensor[0]))\n",
    "        for i in range(len(converted_tensor[0])):\n",
    "            print(type(converted_tensor[0][i]))\n",
    "            # myPlt1 = viz.visualize_image_attr(np.transpose(myImageAttributes.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "            #                             np.transpose(X.squeeze().cpu().detach().numpy(), (1,2,0)), \n",
    "            #                             \"original_image\",\n",
    "            #                             cmap=\"magma\",\n",
    "            #                             #   show_colorbar=True,\n",
    "            #                             # fig_size=(3.56,3.56),\n",
    "            #                             use_pyplot=True\n",
    "            #                             )\n",
    "            \n",
    "            if converted_tensor[0][i] == 1:\n",
    "                print(\"i is: \", i)\n",
    "                print(\"Class is: \", myLabelList[i])\n",
    "                myImageAttributes = myGuidedGradCam.attribute(X, i)\n",
    "                # print(myImageAttributes.shape)\n",
    "                default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                    [(0, '#ffffff'),\n",
    "                                                    (0.25, '#000000'),\n",
    "                                                    (1, '#000000')], N=256)\n",
    "\n",
    "                myPlt = viz.visualize_image_attr(np.transpose(myImageAttributes.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                        np.transpose(imgTransform.squeeze().cpu().detach().numpy(), (1,2,0)), \n",
    "                                        \"heat_map\",\n",
    "                                        cmap=\"magma\",\n",
    "                                        sign=\"all\",\n",
    "                                        #   show_colorbar=True,\n",
    "                                        # fig_size=(3.56,3.56),\n",
    "                                        use_pyplot=True\n",
    "                                        )\n",
    "                \n",
    "\n",
    "                # myPlt1 = viz.visualize_image_attr(np.transpose(myImageAttributes.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                #                         np.transpose(X.squeeze().cpu().detach().numpy(), (1,2,0)), \n",
    "                #                         \"original_image\",\n",
    "                #                         cmap=\"magma\",\n",
    "                #                         #   show_colorbar=True,\n",
    "                #                         # fig_size=(3.56,3.56),\n",
    "                #                         use_pyplot=True\n",
    "                #                         )\n",
    "                \n",
    "                # oriImag = np.transpose(X.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "                # print(\"OriImageShape: \", oriImag.shape)\n",
    "\n",
    "                # fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "                # # Add a subplot to the figure\n",
    "                # ax = fig.add_subplot(111)\n",
    "\n",
    "                # # Display the oriImag using imshow on the subplot\n",
    "                # img_arr = ax.imshow(oriImag)\n",
    "                # # print(img_arr)\n",
    "                # print(\"OrigImage: \", fig.get_size_inches())\n",
    "                # print(\"PltImage: \", myPlt1[0].get_size_inches())\n",
    "                # break\n",
    "\n",
    "                #strtPlotting\n",
    "                fig, axis = plt.subplots(1, 4, figsize=(30,30))\n",
    "\n",
    "                myCv2Img = figArr(myPlt[0])\n",
    "                # cv2.imwrite(\"./tempImagesPres/Heatmap\" + str(idx) + \".jpg\", myCv2Img)\n",
    "                # originalImage = figArr(myPlt1[0])\n",
    "                # cv2.imwrite(\"./tempImagesPres/Original\" + str(idx) + \".jpg\", originalImage)\n",
    "                kernel = np.ones((5, 5), np.uint8)\n",
    "                # dialatedImg = cv2.dilate(myCv2Img, kernel, iterations=1)\n",
    "                axis[0].imshow(cv2.cvtColor(myCv2Img, cv2.COLOR_BGR2RGB))\n",
    "                myGrayImg = cv2.cvtColor(myCv2Img, cv2.COLOR_BGR2GRAY)\n",
    "                myThresh = cv2.threshold(myGrayImg, 30, 255, cv2.THRESH_BINARY)[1]  #+ cv2.THRESH_OTSU\n",
    "                dialatedImg = cv2.dilate(myThresh, kernel, iterations=1)\n",
    "                axis[1].imshow(cv2.cvtColor(dialatedImg, cv2.COLOR_BGR2RGB))\n",
    "                # cv2.imwrite(\"./tempImagesPres/dialated\" + str(idx) + \".jpg\", dialatedImg)\n",
    "                imgCopy = originalImage.copy()\n",
    "                imgCopy1 = myCv2Img.copy()\n",
    "                # edged = cv2.Canny(dialatedImg, 30, 200)\n",
    "                myContours, myHierearchy = cv2.findContours(dialatedImg, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                sortedConts = sorted(myContours, key=cv2.contourArea, reverse=True)\n",
    "                largestContours = sortedConts[0:1]\n",
    "                # myContours = myContours[0] if len(myContours) == 2 else myContours[1]\n",
    "                contDraw = cv2.drawContours(dialatedImg, largestContours, -1, (255, 255, 0), 3)\n",
    "                # axis[2].imshow(cv2.cvtColor(contDraw, cv2.COLOR_BGR2RGB))\n",
    "                for cntr in largestContours:\n",
    "                    if cv2.arcLength(cntr, True) > 300:\n",
    "                        print(\"ARCLENGTH\", cv2.arcLength(cntr, True))\n",
    "                        x,y,w,h = cv2.boundingRect(cntr)\n",
    "                        cv2.rectangle(originalImage, (x, y), (x+w, y+h), (36, 255, 12), 2)\n",
    "                        x2 = x + w\n",
    "                        y2 = y + h\n",
    "                        xc = (x + x2) / 2\n",
    "                        yc = (y + y2) / 2\n",
    "                        nxc = xc / 600\n",
    "                        nyc = yc / 600\n",
    "                        nw = w / 600\n",
    "                        nh = h / 600\n",
    "\n",
    "                        myDict[myLabelList[i]] = (nxc, nyc, nw, nh)\n",
    "                    # print(\"x,y,w,h:\",x,y,w,h)\n",
    "                for cntr in largestContours:\n",
    "                    if cv2.arcLength(cntr, True) > 300:\n",
    "                        print(\"ARCLENGTH\", cv2.arcLength(cntr, True))\n",
    "                        x,y,w,h = cv2.boundingRect(cntr)\n",
    "                        cv2.rectangle(imgCopy1, (x, y), (x+w, y+h), (36, 255, 12), 2)\n",
    "                        print(\"x,y,w,h:\",x,y,w,h)\n",
    "                axis[2].imshow(cv2.cvtColor(imgCopy1, cv2.COLOR_BGR2RGB))\n",
    "                # cv2.imwrite(\"./tempImagesPres/heatmapBox\" + str(idx) + \".jpg\", imgCopy1)\n",
    "                axis[3].imshow(cv2.cvtColor(originalImage, cv2.COLOR_BGR2RGB))\n",
    "                tempImage = originalImage\n",
    "                # cv2.imwrite(\"./tempImagesPres/OriginalBox\" + str(idx) + \".jpg\", imgCopy)\n",
    "\n",
    "                # head, tail = os.path.split(img_path[0])\n",
    "                # if not os.path.isdir(\"./gradCamImages/\" + head):\n",
    "                #     os.makedirs(\"./gradCamImages/\" + head)\n",
    "            else:\n",
    "                print(\"No Class found!\")\n",
    "        print(myDict)\n",
    "        plt.figure(2, figsize=(6,6))\n",
    "        print(\"ORI IMG DIM: \", tempImage.shape)\n",
    "        plt.imshow(cv2.cvtColor(tempImage, cv2.COLOR_BGR2RGB))\n",
    "        print(\"intermediate\")\n",
    "        plt.show()\n",
    "        print(\"afterPlot\")\n",
    "        \n",
    "        if idx == 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'useGradcam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/raid/home/ayrisbud/us-famli-pl/src/generateBBGroundTruth copy.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfamli-gpu1.med.unc.edu/mnt/raid/home/ayrisbud/us-famli-pl/src/generateBBGroundTruth%20copy.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m myLabels \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfamli-gpu1.med.unc.edu/mnt/raid/home/ayrisbud/us-famli-pl/src/generateBBGroundTruth%20copy.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# main(myCSV, myModel, myImgCol, myClassCol, myNn, myOutFile, myBatchSize, myNumWorkers, myMountPoint, myExtractFeatures)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfamli-gpu1.med.unc.edu/mnt/raid/home/ayrisbud/us-famli-pl/src/generateBBGroundTruth%20copy.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m useGradcam(myCSV\u001b[39m=\u001b[39mmyCSV, myModel\u001b[39m=\u001b[39mmyModel, myImgCol \u001b[39m=\u001b[39m myImgCol, myNn\u001b[39m=\u001b[39mmyNn, myOutFile\u001b[39m=\u001b[39mmyOutFile, myBatchSize\u001b[39m=\u001b[39mmyBatchSize, myNumWorkers\u001b[39m=\u001b[39mmyNumWorkers, myMountPoint\u001b[39m=\u001b[39mmyMountPoint, myExtractFeatures\u001b[39m=\u001b[39mmyExtractFeatures, myLabels\u001b[39m=\u001b[39mmyLabels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'useGradcam' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    myCSV = \"/mnt/raid/home/ayrisbud/us-famli-pl/src/annotatedTrainConcise.csv\"\n",
    "    # myModel = \"/mnt/raid/home/ayrisbud/train_output/classification/epoch=21-val_loss=1.06.ckpt\"\n",
    "    myModel = \"/mnt/raid/home/ayrisbud/train_output/classification/epoch=35-val_loss=1.01.ckpt\"\n",
    "    myImgCol = \"img_path\"\n",
    "    myClassCol = \"pred_cluster\"\n",
    "    myNn = \"efficientnet_b0\"    \n",
    "    myOutFile = \"./myOutput\"\n",
    "    myBatchSize = 1\n",
    "    myNumWorkers = 16\n",
    "    myMountPoint = \"/mnt/raid/C1_ML_Analysis/\"\n",
    "    myExtractFeatures = False\n",
    "    myLabels = True\n",
    "    \n",
    "\n",
    "    # main(myCSV, myModel, myImgCol, myClassCol, myNn, myOutFile, myBatchSize, myNumWorkers, myMountPoint, myExtractFeatures)\n",
    "    useGradcam(myCSV=myCSV, myModel=myModel, myImgCol = myImgCol, myNn=myNn, myOutFile=myOutFile, myBatchSize=myBatchSize, myNumWorkers=myNumWorkers, myMountPoint=myMountPoint, myExtractFeatures=myExtractFeatures, myLabels=myLabels)\n",
    "    # useGBackprop(myCSV, myModel, myImgCol, myClassCol, myNn, myOutFile, myBatchSize, myNumWorkers, myMountPoint, myExtractFeatures)\n",
    "    # getBoundingImages(myCSV, myModel, myImgCol, myClassCol, myNn, myOutFile, myBatchSize, myNumWorkers, myMountPoint, myExtractFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_us",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
