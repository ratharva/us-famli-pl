{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/raid/home/ayrisbud/anaconda3/envs/torch_us/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/raid/home/ayrisbud/anaconda3/envs/torch_us/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/mnt/raid/home/ayrisbud/anaconda3/envs/torch_us/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/mnt/raid/home/ayrisbud/anaconda3/envs/torch_us/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/mnt/raid/home/ayrisbud/anaconda3/envs/torch_us/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/mnt/raid/home/ayrisbud/anaconda3/envs/torch_us/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from torch.autograd import Function\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "\n",
    "# from loaders.ultrasound_dataset import USDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from loaders.ultrasound_dataset_classification import USDataset\n",
    "from transforms.ultrasound_transforms import USClassEvalTransforms\n",
    "from transferModel import EfficientNetTransfer\n",
    "# from transforms.ultrasound_transforms import USEvalTransforms\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "from captum.attr import GuidedGradCam, GuidedBackprop\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "from captum.attr import visualization as viz\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from torchvision import transforms\n",
    "from monai.transforms import ScaleIntensityRange\n",
    "\n",
    "from pl_bolts.transforms.dataset_normalizations import (\n",
    "    imagenet_normalization\n",
    ")\n",
    "import nrrd\n",
    "from PIL import Image\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import cv2\n",
    "# from matplotlib.backends.backend_qt4agg import FigureCanvasQTAgg as FigureCanvas\n",
    "from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\n",
    "\n",
    "import pydicom\n",
    "from sklearn.model_selection import train_test_split\n",
    "import uuid\n",
    "import copy\n",
    "from cloneModelArchitectureV3 import EfficientNetClone\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figArr(fig, draw=True):\n",
    "    fig.set_facecolor(\"black\")\n",
    "    myCanvas = fig.canvas#FigureCanvas(fig)\n",
    "    myCanvas.draw()\n",
    "    w, h = myCanvas.get_width_height()#fig.get_size_inches() * fig.get_dpi()\n",
    "    myArr = np.frombuffer(myCanvas.tostring_rgb(), dtype=np.uint8).reshape(h, w, 3)\n",
    "    myImg = cv2.cvtColor(myArr, cv2.COLOR_RGB2BGR)\n",
    "    # plt_fig.grid()\n",
    "    return myImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useGradcam(myCSV, myModel, myImgCol, myLabels, myNn, myOutFile, myBatchSize, myNumWorkers, myMountPoint, myExtractFeatures):\n",
    "    # model = EfficientNetTransfer(base_encoder=myNn, ckpt_path=\"/mnt/raid/C1_ML_Analysis/train_output/classification/extract_frames_blind_sweeps_c1_30082022_wscores_train_train_sample_clean_feat/epoch=9-val_loss=0.27.ckpt\").load_from_checkpoint(myModel)\n",
    "    model = EfficientNetClone(base_encoder=myNn, ckpt_path=\"/mnt/raid/C1_ML_Analysis/train_output/classification/extract_frames_blind_sweeps_c1_30082022_wscores_train_train_sample_clean_feat/epoch=9-val_loss=0.27.ckpt\").load_from_checkpoint(myModel, strict=False)\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    # print(model)\n",
    "    myGuidedGradCam = GuidedGradCam(model, model.efficientnet.convnet.features[8][0])\n",
    "\n",
    "    myLabelList = ['No structures visible', 'Head Visible',\n",
    "                'Abdomen Visible', 'Amniotic fluid visible', \n",
    "                'Placenta visible', 'Fetus or CRL visible']\n",
    "    if myExtractFeatures:\n",
    "            model.extract_features = True\n",
    "\n",
    "    if(os.path.splitext(myCSV)[1] == \".csv\"):        \n",
    "        df_test = pd.read_csv(os.path.join(myMountPoint, myCSV))\n",
    "    else:        \n",
    "        df_test = pd.read_parquet(os.path.join(myMountPoint, myCSV))\n",
    "        \n",
    "    test_ds = USDataset(df_test, label_column = None, img_column=myImgCol, transform=USClassEvalTransforms(), mount_point=myMountPoint)\n",
    "    test_loader = DataLoader(test_ds, batch_size=myBatchSize, shuffle=False, num_workers=myNumWorkers, pin_memory=True, prefetch_factor=4)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "\n",
    "    transforms.CenterCrop(256),\n",
    "    # ScaleIntensityRange(a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0),\n",
    "\n",
    "    ])\n",
    "    predictions = []\n",
    "    probs = []\n",
    "    features = []\n",
    "    pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for idx, X in pbar:\n",
    "        if myLabels:\n",
    "            imgTransform = transform(X)\n",
    "        # print(X)\n",
    "        X = X.cuda().contiguous()   \n",
    "        if myExtractFeatures:\n",
    "            # print(\"Does it enter this if2?\")        \n",
    "            pred, x_f = model(X)    \n",
    "            features.append(x_f.cpu().numpy())\n",
    "        else:\n",
    "            # print(\"Does it enter this else?\")\n",
    "            pred = model(X)\n",
    "        myPredSigmoid = nn.Softmax(dim=1)(pred)\n",
    "        converted_tensor = torch.where(myPredSigmoid >= 0.08, torch.tensor(1), torch.tensor(0))\n",
    "        converted_tensor = converted_tensor.cuda()\n",
    "        converted_tensor = np.array(converted_tensor.cpu())\n",
    "        # print(\"Converted Tensor\", converted_tensor)\n",
    "\n",
    "        isInArray = np.any(converted_tensor == 1)\n",
    "        # print(isInArray)\n",
    "        if isInArray:\n",
    "            oriImag = np.transpose(X.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "            # print(\"OriImageShape: \", oriImag.shape)\n",
    "\n",
    "            fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "            # Add a subplot to the figure\n",
    "            ax = fig.add_subplot(111)\n",
    "\n",
    "            # Display the oriImag using imshow on the subplot\n",
    "            img_arr = ax.imshow(oriImag)\n",
    "            originalImage = figArr(fig)\n",
    "            tempImage = copy.deepcopy(originalImage)\n",
    "            myDict = {}\n",
    "            myYoloList = []\n",
    "        # print(\"Length of convertedArray: \", len(converted_tensor[0]))\n",
    "        # converted_tensor[0] = converted_tensor[0][1:]\n",
    "        myUUID = uuid.uuid4()\n",
    "        for i in range(len(converted_tensor[0])):\n",
    "            # print(type(converted_tensor[0][i]))\n",
    "            # if converted_tensor[0][0] == 1:\n",
    "            #     continue\n",
    "            # if converted_tensor[0][5] == 1:\n",
    "            #     continue\n",
    "            \n",
    "            if converted_tensor[0][i] == 1:\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                if i == 5:\n",
    "                    continue\n",
    "                # print(\"i is: \", i)\n",
    "                # print(\"Class is: \", myLabelList[i])\n",
    "                myImageAttributes = myGuidedGradCam.attribute(X, i)\n",
    "                if np.count_nonzero(myImageAttributes) == 0:\n",
    "                    continue\n",
    "                # print(\"Image Attributes are: \", np.count_nonzero(myImageAttributes))\n",
    "                # cv2.imwrite(\"/mnt/raid/home/ayrisbud/USOD/images/train/whatImage.png\", tempImage)\n",
    "                # print(myImageAttributes.shape)\n",
    "                default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                    [(0, '#ffffff'),\n",
    "                                                    (0.25, '#000000'),\n",
    "                                                    (1, '#000000')], N=256)\n",
    "\n",
    "                myPlt = viz.visualize_image_attr(np.transpose(myImageAttributes.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                        np.transpose(imgTransform.squeeze().cpu().detach().numpy(), (1,2,0)), \n",
    "                                        \"heat_map\",\n",
    "                                        cmap=\"magma\",\n",
    "                                        # sign=\"absolute_value\",\n",
    "                                        #   show_colorbar=True,\n",
    "                                        # fig_size=(3.56,3.56),\n",
    "                                        use_pyplot=True\n",
    "                                        )\n",
    "                \n",
    "                # fig, axis = plt.subplots(1, 4, figsize=(30,30))\n",
    "\n",
    "                myCv2Img = figArr(myPlt[0])\n",
    "                # cv2.imwrite(\"./tempImagesPres/Heatmap\" + str(idx) + \".jpg\", myCv2Img)\n",
    "                # originalImage = figArr(myPlt1[0])\n",
    "                # cv2.imwrite(\"./tempImagesPres/Original\" + str(idx) + \".jpg\", originalImage)\n",
    "                kernel = np.ones((5, 5), np.uint8)\n",
    "                # dialatedImg = cv2.dilate(myCv2Img, kernel, iterations=1)\n",
    "                # plt.axis('off')\n",
    "                # axis[0].imshow(cv2.cvtColor(myCv2Img, cv2.COLOR_BGR2RGB))\n",
    "                myGrayImg = cv2.cvtColor(myCv2Img, cv2.COLOR_BGR2GRAY)\n",
    "                myThresh = cv2.threshold(myGrayImg, 30, 255, cv2.THRESH_BINARY)[1]  #+ cv2.THRESH_OTSU\n",
    "                dialatedImg = cv2.dilate(myThresh, kernel, iterations=1)\n",
    "                # plt.axis('off')\n",
    "                # axis[1].imshow(cv2.cvtColor(dialatedImg, cv2.COLOR_BGR2RGB))\n",
    "                # cv2.imwrite(\"./tempImagesPres/dialated\" + str(idx) + \".jpg\", dialatedImg)\n",
    "                imgCopy = originalImage.copy()\n",
    "                imgCopy1 = myCv2Img.copy()\n",
    "                # edged = cv2.Canny(dialatedImg, 30, 200)\n",
    "                myContours, myHierearchy = cv2.findContours(dialatedImg, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                sortedConts = sorted(myContours, key=cv2.contourArea, reverse=True)\n",
    "                largestContours = sortedConts[0:1]\n",
    "                # myContours = myContours[0] if len(myContours) == 2 else myContours[1]\n",
    "                contDraw = cv2.drawContours(dialatedImg, largestContours, -1, (255, 255, 0), 3)\n",
    "                # axis[2].imshow(cv2.cvtColor(contDraw, cv2.COLOR_BGR2RGB))\n",
    "                for cntr in largestContours:\n",
    "                    if cv2.arcLength(cntr, True) > 300:\n",
    "                        # print(\"ARCLENGTH\", cv2.arcLength(cntr, True))\n",
    "                        x,y,w,h = cv2.boundingRect(cntr)\n",
    "                        cv2.rectangle(originalImage, (x, y), (x+w, y+h), (36, 255, 12), 2)\n",
    "                        x2 = x + w\n",
    "                        y2 = y + h\n",
    "                        xc = (x + x2) / 2\n",
    "                        yc = (y + y2) / 2\n",
    "                        nxc = xc / originalImage.shape[0]\n",
    "                        nyc = yc / originalImage.shape[1]\n",
    "                        nw = w / 600\n",
    "                        nh = h / 600\n",
    "                        datFormat = f\"{i} {nxc} {nyc} {nw} {nh}\"\n",
    "                        # print(\"YOLO FormatData: \", datFormat)\n",
    "                        myYoloList.append(datFormat)\n",
    "                        myDict[myLabelList[i]] = (nxc, nyc, nw, nh)\n",
    "                    # print(\"x,y,w,h:\",x,y,w,h)\n",
    "                for cntr in largestContours:\n",
    "                    if cv2.arcLength(cntr, True) > 300:\n",
    "                        # print(\"ARCLENGTH\", cv2.arcLength(cntr, True))\n",
    "                        x,y,w,h = cv2.boundingRect(cntr)\n",
    "                        cv2.rectangle(imgCopy1, (x, y), (x+w, y+h), (36, 255, 12), 2)\n",
    "                        # print(\"x,y,w,h:\",x,y,w,h)\n",
    "                # plt.axis('off')\n",
    "                # axis[2].imshow(cv2.cvtColor(imgCopy1, cv2.COLOR_BGR2RGB))\n",
    "                # cv2.imwrite(\"./tempImagesPres/heatmapBox\" + str(idx) + \".jpg\", imgCopy1)\n",
    "                # plt.axis('off')\n",
    "                # axis[3].imshow(cv2.cvtColor(originalImage, cv2.COLOR_BGR2RGB))\n",
    "                # tempImage = originalImage\n",
    "                # cv2.imwrite(\"./tempImagesPres/OriginalBox\" + str(idx) + \".jpg\", imgCopy)\n",
    "\n",
    "                # head, tail = os.path.split(img_path[0])\n",
    "                # if not os.path.isdir(\"./gradCamImages/\" + head):\n",
    "                #     os.makedirs(\"./gradCamImages/\" + head)\n",
    "            else:\n",
    "                print(\"No Class found!\")\n",
    "        # print(myDict)\n",
    "        # print(myYoloList)\n",
    "        # plt.figure(2, figsize=(6,6))\n",
    "        # print(\"ORI IMG DIM: \", originalImage.shape)\n",
    "        # plt.imshow(cv2.cvtColor(originalImage, cv2.COLOR_BGR2RGB))\n",
    "        # print(\"intermediate\")\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "        # print(\"afterPlot\")\n",
    "        # plt.figure(3, figsize=(6,6))\n",
    "        # print(\"ORI IMG DIM: \", tempImage.shape)\n",
    "        # plt.imshow(cv2.cvtColor(tempImage, cv2.COLOR_BGR2RGB))\n",
    "        # print(\"intermediate\")\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "        # print(\"afterPlot\")\n",
    "        if len(myYoloList) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            cv2.imwrite(\"/mnt/raid/home/ayrisbud/USODClone/images/val/\" + str(myUUID) + \".png\", tempImage)\n",
    "            pathToFile = \"/mnt/raid/home/ayrisbud/USODClone/labels/val/\" + str(myUUID) + \".txt\"\n",
    "            file = open(pathToFile, \"w+\")\n",
    "            for item in myYoloList:\n",
    "                file.write(item + \"\\n\")\n",
    "            file.close()\n",
    "        \n",
    "        # if idx == 2:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    myCSV = \"/mnt/raid/home/ayrisbud/us-famli-pl/src/annotatedValConcise.csv\"\n",
    "    # myModel = \"/mnt/raid/home/ayrisbud/train_output/classification/epoch=21-val_loss=1.06.ckpt\"\n",
    "    # myModel = \"/mnt/raid/home/ayrisbud/train_output/classification/epoch=35-val_loss=1.01.ckpt\"\n",
    "    myModel = \"/mnt/raid/home/ayrisbud/train_output/classification/clone/epoch=20-val_loss=0.418.ckpt\"\n",
    "    myImgCol = \"img_path\"\n",
    "    myClassCol = \"pred_cluster\"\n",
    "    myNn = \"efficientnet_b0\"    \n",
    "    myOutFile = \"./myOutput\"\n",
    "    myBatchSize = 1\n",
    "    myNumWorkers = 16\n",
    "    myMountPoint = \"/mnt/raid/C1_ML_Analysis/\"\n",
    "    myExtractFeatures = False\n",
    "    myLabels = True\n",
    "    \n",
    "\n",
    "    # main(myCSV, myModel, myImgCol, myClassCol, myNn, myOutFile, myBatchSize, myNumWorkers, myMountPoint, myExtractFeatures)\n",
    "    useGradcam(myCSV=myCSV, myModel=myModel, myImgCol = myImgCol, myNn=myNn, myOutFile=myOutFile, myBatchSize=myBatchSize, myNumWorkers=myNumWorkers, myMountPoint=myMountPoint, myExtractFeatures=myExtractFeatures, myLabels=myLabels)\n",
    "    # useGBackprop(myCSV, myModel, myImgCol, myClassCol, myNn, myOutFile, myBatchSize, myNumWorkers, myMountPoint, myExtractFeatures)\n",
    "    # getBoundingImages(myCSV, myModel, myImgCol, myClassCol, myNn, myOutFile, myBatchSize, myNumWorkers, myMountPoint, myExtractFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_us",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
